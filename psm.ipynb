{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The effect of comments on suicidal posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Installed in requirements, if not install here.\n",
    "# !pip install dowhy==0.5.1\n",
    "# !pip install nltk\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import dowhy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from IPython.display import Image, display\n",
    "logging.getLogger(\"dowhy\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "output_dir = './data/output/psm/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "df = pd.DataFrame(\n",
    "{'treatment':np.random.normal(0,1,100), 'outcome':np.random.normal(0,1,100)}\n",
    ")\n",
    "\n",
    "\n",
    "causal_graph = \"\"\"digraph {\n",
    "treatment[label=\"Treatment\"];\n",
    "outcome[label=\"Outcome\"];\n",
    "confounder[label = \"Confounder\"];\n",
    "covariate[label = \"Covariate\"];\n",
    "\n",
    "U[label=\"Unobserved Confounders\"];\n",
    "U -> treatment;\n",
    "U -> outcome;\n",
    "\n",
    "treatment -> outcome;\n",
    "covariate -> outcome;\n",
    "confounder -> treatment;\n",
    "confounder -> outcome;\n",
    "\n",
    "\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "model= dowhy.CausalModel(\n",
    "        data = df,\n",
    "        graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "    \n",
    "        treatment='treatment',\n",
    "        outcome='outcome')\n",
    "model.view_model()\n",
    "\n",
    "display(Image(filename=\"causal_model.png\"))\n",
    "plt.savefig(output_dir+'dag_basic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "{'treatment':np.random.normal(0,1,100), 'outcome':np.random.normal(0,1,100)}\n",
    ")\n",
    "\n",
    "causal_graph = \"\"\"digraph {\n",
    "outcome[label=\"Future SW posts (Y)\"];\n",
    "\n",
    "vector[label=\"SW post's semantic\\\\nvariables (X)\"];\n",
    "vector -> outcome;\n",
    "vector -> num_comments_bin;\n",
    "\n",
    "num_comments_bin[label=\"Treatment (T)\"];\n",
    "num_comments_bin -> outcome;\n",
    "\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "# U[label=\"Unobserved Confounders\"];\n",
    "# U -> num_comments_bin;\n",
    "# U -> outcome;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model= dowhy.CausalModel(\n",
    "        data = df,\n",
    "        graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "        treatment='num_comments_bin',\n",
    "        outcome='outcome')\n",
    "\n",
    "\n",
    "name = 'causal_model.png'\n",
    "model.view_model()\n",
    "display(Image(filename=name,width = 150))\n",
    "# img = Image(filename=name)\n",
    "\n",
    "# with open('./data/output/figures/'+name, \"w\") as png:\n",
    "#     png.write(img)\n",
    "# Image.save(output_dir+'dag1.ong', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "{'treatment':np.random.normal(0,1,100), 'outcome':np.random.normal(0,1,100)}\n",
    ")\n",
    "\n",
    "causal_graph = \"\"\"digraph {\n",
    "outcome[label=\"Future\\\\nSW posts\"];\n",
    "\n",
    "vector[label=\"SW post\\\\nvariables\"];\n",
    "vector -> outcome;\n",
    "vector -> num_comments_bin;\n",
    "\n",
    "num_comments_bin[label=\"Treatment\"];\n",
    "num_comments_bin -> outcome;\n",
    "\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "# U[label=\"Unobserved Confounders\"];\n",
    "# U -> num_comments_bin;\n",
    "# U -> outcome;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model= dowhy.CausalModel(\n",
    "        data = df,\n",
    "        graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "        treatment='num_comments_bin',\n",
    "        outcome='outcome')\n",
    "\n",
    "\n",
    "name = 'causal_model.png'\n",
    "model.view_model()\n",
    "display(Image(filename=name,width = 250))\n",
    "# img = Image(filename=name)\n",
    "\n",
    "# with open('./data/output/figures/'+name, \"w\") as png:\n",
    "#     png.write(img)\n",
    "# Image.save(output_dir+'dag1.ong', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG from De Choudhury and Kiciman (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "{'treatment':np.random.normal(0,1,100), 'outcome':np.random.normal(0,1,100)}\n",
    ")\n",
    "\n",
    "\n",
    "# Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "\n",
    "causal_graph = \"\"\"digraph {\n",
    "\n",
    "treatment[label=\"Treatment comment ngram\"];\n",
    "outcome[label=\"SuicideWatch/MentalHealth\"];\n",
    "treatment -> outcome;\n",
    "\n",
    "U[label=\"Unobserved Confounders\"];\n",
    "U -> treatment;\n",
    "U -> outcome;\n",
    "\n",
    "prior_ngram_0[label = \"prior_ngram_0\"];\n",
    "prior_ngram_0 -> outcome;\n",
    "\n",
    "prior_ngram_1[label = \"prior_ngram_1\"];\n",
    "prior_ngram_1 -> outcome;\n",
    "\n",
    "prior_ngram_n[label = \"prior_ngram_n\"];\n",
    "prior_ngram_n -> outcome;\n",
    "\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "model= dowhy.CausalModel(\n",
    "        data = df,\n",
    "        graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "        treatment='treatment',\n",
    "        outcome='outcome')\n",
    "model.view_model()\n",
    "\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "{'treatment':np.random.normal(0,1,100), 'outcome':np.random.normal(0,1,100)}\n",
    ")\n",
    "\n",
    "\n",
    "# Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "\n",
    "causal_graph = \"\"\"digraph {\n",
    "\n",
    "treatment[label=\"Treatment comment ngram\"];\n",
    "outcome[label=\"SITB/nonSITB\"];\n",
    "treatment -> outcome;\n",
    "\n",
    "U[label=\"Unobserved Confounders\"];\n",
    "U -> treatment;\n",
    "U -> outcome;\n",
    "\n",
    "post_meaning_0\n",
    "post_meaning_0 -> treatment;\n",
    "post_meaning_0 -> outcome;\n",
    "\n",
    "num_comments[label = \"# comments\"];\n",
    "num_comments -> outcome;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comment_ngram_0[label = \"comment_ngram_0\"];\n",
    "comment_ngram_0 -> outcome;\n",
    "\n",
    "comment_ngram_1[label = \"comment_ngram_1\"];\n",
    "comment_ngram_1 -> outcome;\n",
    "\n",
    "comment_ngram_n[label = \"comment_ngram_n\"];\n",
    "comment_ngram_n -> outcome;\n",
    "\n",
    "post_sentiment -> comment_ngram_0\n",
    "post_sentiment -> comment_ngram_1\n",
    "post_sentiment -> comment_ngram_n\n",
    "\n",
    "\n",
    "user_profile[label = \"User profile\"];\n",
    "user_profile -> outcome;\n",
    "user_profile -> post_ngram_0\n",
    "user_profile -> post_ngram_1\n",
    "user_profile -> post_ngram_n\n",
    "user_profile -> post_sentiment\n",
    "\n",
    "post_ngram_0[label = \"post_ngram_0\"];\n",
    "post_ngram_0 -> outcome;\n",
    "\n",
    "post_ngram_1[label = \"post_ngram_1\"];\n",
    "post_ngram_1 -> outcome;\n",
    "\n",
    "post_ngram_n[label = \"post_ngram_n\"];\n",
    "post_ngram_n -> outcome;\n",
    "\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "model= dowhy.CausalModel(\n",
    "        data = df,\n",
    "        graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "        treatment='treatment',\n",
    "        outcome='outcome')\n",
    "model.view_model()\n",
    "\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{n} \\sum \\limits _{i=1} ^{n} P(outcome=1 | confounder_{i}, treatment=1) - P(outcome=1 | confounder_{i}, treatment=0) > 0$\n",
    "\n",
    "$\\frac{1}{n} \\sum \\limits _{i=1} ^{n} P(outcome=1 | confounder_{i}, treatment=1) - P(outcome=1 | confounder_{i}, treatment=0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and dataset to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "first_sw = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "\n",
    "comments = pd.read_csv(input_dir+'first_sw_comments_liwc2015_2021-07-20-18-52-35.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw[first_sw.id=='avhnbp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single submission was replied by about 10 comments all containing many repetitions of \"fuck off\" so that n gram is very frequent in the corpus but just because of a single post. i;ll remove\n",
    "print(first_sw.shape)\n",
    "print(comments.shape)\n",
    "first_sw = first_sw[first_sw['id']!='avhnbp']\n",
    "comments = comments[comments.link_id!='t3_avhnbp']\n",
    "print(first_sw.shape)\n",
    "print(comments.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove recent posts to guarantee we have posts for a year: not necessary, last post is from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw.created_utc_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descriptive stats on comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_count = first_sw.num_comments.value_counts().reset_index().sort_values('index')\n",
    "comments_count['num_comments %'] = comments_count['num_comments'] /comments_count['num_comments'].sum()\n",
    "comments_count.columns = ['Comments', 'Posts N', 'Posts %']\n",
    "\n",
    "print(first_sw.num_comments.describe().round(1))\n",
    "comments_count.round(2).sort_values('Comments').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_sw_cols = [n for n in first_sw.columns if 'subsequent_sw_' in n]\n",
    "\n",
    "first_sw[subsequent_sw_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw[subsequent_sw_cols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_cols =first_sw[subsequent_sw_cols]\n",
    "sw_cols_binary = sw_cols[sw_cols[subsequent_sw_cols]==0].fillna(1)\n",
    "sw_cols_binary.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_xsmall = ['WC', \n",
    "#         'Analytic', 'Clout', 'Authentic', \n",
    "#         'Tone', \n",
    "        \n",
    "#         'WPS', 'Sixltr',\n",
    "#        'Dic', 'function', \n",
    "#         'pronoun', \n",
    "        'ppron', \n",
    "        'i', \n",
    "#         'we', 'you', 'shehe',\n",
    "#        'they', 'ipron', \n",
    "#         'article', 'prep', 'auxverb', \n",
    "#         'adverb', \n",
    "#         'conj',\n",
    "       'negate', \n",
    "#         'verb', \n",
    "#         'adj', \n",
    "        'compare', \n",
    "        'interrog', \n",
    "#         'number', \n",
    "#         'quant',\n",
    "        \n",
    "       'affect', \n",
    "#         'posemo', \n",
    "#         'negemo', \n",
    "        'anx', 'anger', 'sad', \n",
    "        \n",
    "#         'social',\n",
    "        'family', 'friend', \n",
    "#         'female', 'male', \n",
    "#         'cogproc', \n",
    "        'insight',\n",
    "        'cause', \n",
    "        'discrep', \n",
    "        'tentat', \n",
    "        'certain', \n",
    "#         'differ', \n",
    "        \n",
    "        'percept',\n",
    "#        'see', 'hear', 'feel', \n",
    "        \n",
    "        \n",
    "#         'bio', \n",
    "        'body', 'health', 'sexual', 'ingest',\n",
    "        \n",
    "#        'drives', \n",
    "#         'affiliation', \n",
    "        'achieve', 'power', 'reward', 'risk',\n",
    "        \n",
    "       'focuspast', 'focuspresent', 'focusfuture', \n",
    "\n",
    "#         'relativ', 'motion',\n",
    "#        'space', \n",
    "#         'time', \n",
    "        \n",
    "        'work', 'leisure', 'home', \n",
    "        'money', \n",
    "        'relig',\n",
    "        \n",
    "       'death', \n",
    "        'informal', \n",
    "        'swear', \n",
    "#         'netspeak', 'assent', 'nonflu',\n",
    "        \n",
    "#        'filler', 'AllPunc', 'Period', 'Comma', 'Colon', 'SemiC', \n",
    "        'QMark',\n",
    "       'Exclam', \n",
    "        \n",
    "#         'Dash', 'Quote', 'Apostro', 'Parenth', 'OtherP'\n",
    "       ]\n",
    "\n",
    "\n",
    "print(len(liwc_xsmall))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_psm(dataset, add_variable=None, treatment_cutoff = None, \n",
    "                 method = \"backdoor.propensity_score_stratification\", \n",
    "                 refute_methods = [\"random_common_cause\", \"placebo_treatment_refuter\", \"data_subset_refuter\"]):\n",
    "    '''\n",
    "    \"placebo_treatment_refuter\", \"data_subset_refuter\" TAKE 1 min...\n",
    "    '''\n",
    "    \n",
    "    # Important: always use doubel quotation markes (\"\"), not single ('')\n",
    "\n",
    "    causal_graph = \"\"\"digraph {\n",
    "    outcome[label=\"SW+/SW-\"];\n",
    "\n",
    "    treatment[label=\"treatment\"];\n",
    "    treatment -> outcome;\n",
    "\n",
    "    U[label=\"Unobserved Confounders\"];\n",
    "    U -> treatment;\n",
    "    U -> outcome;\n",
    "\n",
    "\n",
    "    }\"\"\"\n",
    "\n",
    "    # Replace 1 and 0 by True and False, I think this is needed.\n",
    "    dataset.treatment =     dataset.treatment .astype(str)\n",
    "    dataset=    dataset.replace({'treatment': {'0': False, '1': True}})\n",
    "    # Add all confounders\n",
    "    if add_variable:\n",
    "        for variable in add_variable:\n",
    "            variable_graph = f\"\"\"\n",
    "            {variable}[label=\"{variable}\"];\n",
    "            {variable} -> outcome;\n",
    "            {variable} -> treatment;\n",
    "            \"\"\"\n",
    "\n",
    "            causal_graph = causal_graph.replace('}',variable_graph+\"}\")\n",
    "\n",
    "    model= dowhy.CausalModel(\n",
    "            data = dataset,\n",
    "            graph=causal_graph.replace(\"\\n\", \" \"),\n",
    "            treatment='treatment',\n",
    "            outcome='outcome')\n",
    "\n",
    "\n",
    "    print('Step 2: Identify the causal effect=============================')\n",
    "    #Identify the causal effect\n",
    "    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "    print(identified_estimand)\n",
    "    print('============================================================')\n",
    "\n",
    "\n",
    "\n",
    "    print('Step 3: ATE=============================')\n",
    "    estimate = model.estimate_effect(identified_estimand, \n",
    "                                     method_name=method,target_units=\"ate\"\n",
    "                                    )\n",
    "    \n",
    "    print('Step 4: Refute=============================')\n",
    "    refute_results = {}\n",
    "    for refute_method in refute_methods:\n",
    "        result = model.refute_estimate(identified_estimand, estimate,\n",
    "            method_name=refute_method)\n",
    "        if refute_method == 'random_common_cause':\n",
    "#             https://github.com/microsoft/dowhy/blob/e907134938aee6d0b3ceec2492e14d86692d2841/dowhy/causal_refuter.py#L215\n",
    "            result_value = result.new_effect\n",
    "            refute_results[refute_method]= result_value\n",
    "        elif refute_method in [\"placebo_treatment_refuter\", \"data_subset_refuter\"]:\n",
    "            result_value1 = result.new_effect\n",
    "            result_value2 = result.refutation_result.get('p_value')\n",
    "            refute_results[refute_method]= result_value1\n",
    "            refute_results[refute_method+'_p-value']= result_value2\n",
    "\n",
    "    \n",
    "    # ATE = Average Treatment Effect\n",
    "    # ATT = Average Treatment Effect on Treated (i.e. those who were assigned a different room)\n",
    "    # ATC = Average Treatment Effect on Control (i.e. thse who were not assigned a different room)\n",
    "    print(estimate)\n",
    "    print(refute_results)\n",
    "#     print(estimate.value)\n",
    "\n",
    "    print('============================================================')\n",
    "    \n",
    "    return model, identified_estimand, estimate, estimate.value, refute_results\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments.shape)\n",
    "comments = comments.dropna(subset=['link_id'])\n",
    "print(comments.shape)\n",
    "comments = comments.dropna(subset=['body'])\n",
    "print(comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove posts without comments here\n",
    "print(first_sw.shape)\n",
    "print(comments.shape)\n",
    "# first_sw = first_sw[first_sw['id'].isin(comments.link_id_clean.values)] #each SW post is in comment's link_id \n",
    "\n",
    "# print(first_sw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of amount of comments on outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_count = first_sw.num_comments.value_counts().reset_index().sort_values('index')\n",
    "comments_count['num_comments %'] = comments_count['num_comments'] /comments_count['num_comments'].sum()\n",
    "comments_count.columns = ['Comments', 'Posts N', 'Posts %']\n",
    "\n",
    "print(first_sw.num_comments.describe().round(1))\n",
    "comments_count.round(2).sort_values('Comments').iloc[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_comments = comments_count['Posts N'].sum()\n",
    "total_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_names = ['financial_stress',\n",
    "       'domestic_stress_and_violence', 'loneliness_isolation', 'substance_use',\n",
    "       'suicidality_general', 'suicidality_selfharm', 'suicidality_passive',\n",
    "       'suicidality_active', 'mental_health']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_i =\"backdoor.propensity_score_stratification\"# \"backdoor.propensity_score_matching\" #\"backdoor.propensity_score_stratification\"#, \"backdoor.propensity_score_matching\"\n",
    "outcome_days = [7,14,21,30,60,90,180,365]\n",
    "amount_of_comments = range(1,11) #range(1,11) #treatment\n",
    "\n",
    "refute_methods = [\"random_common_cause\",\"data_subset_refuter\"]#,\"placebo_treatment_refuter\"]# \"data_subset_refuter\"]\n",
    "covariates = liwc_xsmall+ lexicon_names#liwc_small+[f'num_comments']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_comments = amount_of_comments[-1]\n",
    "\n",
    "num_comments_support = []\n",
    "\n",
    "for num_comment_i in [0]+list(amount_of_comments):\n",
    "    dataset = first_sw.copy()\n",
    "\n",
    "    # reduce dataset to those containg 0 and i comments\n",
    "    if num_comment_i == max_comments:\n",
    "        dataset = dataset[(dataset.num_comments==0) | (dataset.num_comments>=num_comment_i)]\n",
    "        num_comments_support.append(\n",
    "        [str(num_comment_i)+'+', dataset[(dataset.num_comments>=num_comment_i)].shape[0]])\n",
    "    else:\n",
    "        dataset = dataset[(dataset.num_comments==0) | (dataset.num_comments==num_comment_i)]\n",
    "        num_comments_support.append(\n",
    "            [str(num_comment_i), dataset[(dataset.num_comments==num_comment_i)].shape[0]])\n",
    "    \n",
    "print('Posts receiving 0 comments: ', dataset[(dataset.num_comments==0)].shape[0])\n",
    "num_comments_support= pd.DataFrame(num_comments_support, columns = ['n','Posts receiving n comments'])\n",
    "\n",
    "num_comments_support['%'] = (num_comments_support['Posts receiving n comments'].values/total_comments).round(2)\n",
    "num_comments_support = num_comments_support.set_index('n')\n",
    "num_comments_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_comments_support.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build dataset\n",
    "\n",
    "objects = {}\n",
    "results = []\n",
    "\n",
    "\n",
    "# Build dataset \n",
    "\n",
    "\n",
    "\n",
    "# for outcome_day in [7,14,21,30,60]:\n",
    "#     for num_comment_i in [1,2,3,4, 5,6,7,8]:\n",
    "\n",
    "\n",
    "max_comments = amount_of_comments[-1]\n",
    "for outcome_day in outcome_days:\n",
    "    for num_comment_i in amount_of_comments:\n",
    "\n",
    "        dataset = first_sw.copy()\n",
    "\n",
    "        # reduce dataset to those containg 0 and i comments\n",
    "        if num_comment_i == max_comments:\n",
    "            dataset = dataset[(dataset.num_comments==0) | (dataset.num_comments>=num_comment_i)]\n",
    "        else:\n",
    "            dataset = dataset[(dataset.num_comments==0) | (dataset.num_comments==num_comment_i)]\n",
    "\n",
    "        dataset['outcome'] = first_sw[f'subsequent_sw_{outcome_day}days'] #I will repeat for all time windows\n",
    "        dataset['treatment'] = [0 if n==0 else 1 for n in dataset['num_comments'].values] #you re not alone\n",
    "        keep_cols = ['outcome']+['treatment']+covariates\n",
    "        dataset = dataset[keep_cols]\n",
    "        didnt_work = 0\n",
    "        treatment_i = 'comments'\n",
    "\n",
    "        #Change treatment\n",
    "        # Estimate     \n",
    "        model, identified_estimand, estimate, estimate_value, refute_results = estimate_psm(\n",
    "            dataset, \n",
    "            add_variable=covariates,\n",
    "            method = method_i,\n",
    "            refute_methods = refute_methods,\n",
    "        )\n",
    "        #Save\n",
    "        treatment_name = f'{treatment_i}_{num_comment_i}'\n",
    "        objects[treatment_name] = [model, identified_estimand, estimate]\n",
    "        \n",
    "        coefs = pd.DataFrame(estimate.estimator._propensity_score_model.coef_,\n",
    "                         index=[treatment_name], \n",
    "                         columns=covariates).T.sort_values(treatment_name).T\n",
    "        coefs['estimate_value'] = estimate_value\n",
    "        coefs['treatment_count'] = num_comment_i\n",
    "        coefs['outcome_days'] = outcome_day\n",
    "        coefs['support_0'] = dataset[dataset.treatment==0].shape[0]\n",
    "        coefs['support_num_comment_i'] = dataset[dataset.treatment!=0].shape[0]\n",
    "        for key, value in refute_results.items():\n",
    "            coefs[key] = value\n",
    "\n",
    "        results.append(coefs)\n",
    "\n",
    "\n",
    "results = pd.concat(results)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def gen_timestamp():\n",
    "    timestamp = '{:%Y-%m-%dT%H-%M-%S}'.format(datetime.datetime.now())\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_dict(d, filepath='./dict_name'):\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(json.dumps(d))\n",
    "\n",
    "def load_dict(filepath='./dict_name'):\n",
    "    with open(filepath) as f:\n",
    "        d = json.loads(f.read())\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_str = str(method_i.split('.')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefs['support_0'] = dataset[dataset.treatment==0].shape[0]\n",
    "\n",
    "\n",
    "results.to_csv(output_dir+f'comments/results_{method_str}_{outcome_days}days_{amount_of_comments}comments_{gen_timestamp()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['estimate_value', 'outcome_days', 'treatment_count', 'support_0', 'support_num_comment_i']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comments = pd.read_csv(output_dir+'comments/results_propensity_score_stratification_[7, 14, 21, 30, 60, 90, 180, 365]days_range(1, 11)comments_2021-07-31T14-16-27.csv', index_col = 0)\n",
    "results_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_xsmall = ['WC',\n",
    " 'ppron',\n",
    " 'i',\n",
    " 'negate',\n",
    " 'compare',\n",
    " 'interrog',\n",
    " 'affect',\n",
    " 'anx',\n",
    " 'anger',\n",
    " 'sad',\n",
    " 'family',\n",
    " 'friend',\n",
    " 'insight',\n",
    " 'cause',\n",
    " 'discrep',\n",
    " 'tentat',\n",
    " 'certain',\n",
    " 'percept',\n",
    " 'body',\n",
    " 'health',\n",
    " 'sexual',\n",
    " 'ingest',\n",
    " 'achieve',\n",
    " 'power',\n",
    " 'reward',\n",
    " 'risk',\n",
    " 'focuspast',\n",
    " 'focuspresent',\n",
    " 'focusfuture',\n",
    " 'work',\n",
    " 'leisure',\n",
    " 'home',\n",
    " 'money',\n",
    " 'relig',\n",
    " 'death',\n",
    " 'informal',\n",
    " 'swear',\n",
    " 'QMark',\n",
    " 'Exclam']\n",
    "lexicon_names = ['financial_stress',\n",
    "       'domestic_stress_and_violence', 'loneliness_isolation', 'substance_use',\n",
    "       'suicidality_general', 'suicidality_selfharm', 'suicidality_passive',\n",
    "       'suicidality_active', 'mental_health']\n",
    "\n",
    "\n",
    "covariates = liwc_xsmall+ lexicon_names#\n",
    "\n",
    "outcome_days = [7,14,21,30,60,90,180,365]\n",
    "amount_of_comments = range(1,11) #range(1,11) #treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_counts = results_comments[covariates].mean().sort_values()[::-1].reset_index().round(3)\n",
    "comment_counts.columns = ['Confounder', 'Coefficient']\n",
    "comment_counts_pos = comment_counts[comment_counts.Coefficient>0].iloc[:16]\n",
    "comment_counts_pos.index=range(1,17)\n",
    "\n",
    "print(comment_counts_pos.to_latex())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_col = 'data_subset_refuter'\n",
    "error_delta = results_comments.estimate_value - results_comments[error_col]\n",
    "results_comments[error_col+'_delta'] = error_delta\n",
    "results_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comments.estimate_value.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comments.estimate_value.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = results_comments.groupby('outcome_days').describe()[error_col+'_delta'].round(3)\n",
    "\n",
    "ms = stats['mean'].values\n",
    "stds = stats['std'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'; '.join([f'{d} days: {m} ({s})' for d,m,s in zip(outcome_days,ms, stds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comments.groupby('treatment_count').mean()[error_col+'_delta'].round(3).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare first and second SW posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_dir = './data/input/'\n",
    "output_dir = './data/input/psm/'\n",
    "\n",
    "first = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "second = pd.read_csv(input_dir+'second_sw_submission_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = first[first.author.isin(second.author.values)]\n",
    "second = second[second.author.isin(first.author.values)]\n",
    "print(first.shape, second.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = first.sort_values('author')\n",
    "second = second.sort_values('author')\n",
    "first.author.values == second.author.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Out of 8519 who posted in SW again, we compared first and second post on custom features surrounding suicidal thoughts and behaviors and mental health.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_names = ['financial_stress',\n",
    " 'domestic_stress_and_violence',\n",
    " 'loneliness_isolation',\n",
    " 'substance_use',\n",
    " 'suicidality_general',\n",
    " 'suicidality_selfharm',\n",
    " 'suicidality_passive',\n",
    " 'suicidality_active',\n",
    " 'mental_health']\n",
    "\n",
    "delta = {}\n",
    "delta['author'] = first.author.values\n",
    "for name in lexicon_names:\n",
    "    delta[name] = second[name].values - first[name].values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(delta).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = pd.DataFrame(delta)\n",
    "delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './data/output/psm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import seaborn as sns\n",
    "[np.round(np.mean(n),4) for n in values_across_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name in lexicon_names:\n",
    "    p_values = []\n",
    "    values_across_comments = []\n",
    "    for comments in range(1,11):\n",
    "        if comments == 10:\n",
    "            # obtain first posts that received n comments     \n",
    "            first_i = first[first.num_comments >= comments]            \n",
    "        else:\n",
    "            # obtain first posts that received n comments     \n",
    "            first_i = first[first.num_comments == comments]\n",
    "        # obtain delta (first-second) for those posts     \n",
    "        \n",
    "        delta_df_i = delta_df[delta_df.author.isin(first_i.author.values)]\n",
    "        delta_values = delta_df_i.drop('author',axis=1)[name].values\n",
    "        values_across_comments.append(delta_values)\n",
    "        \n",
    "        # Paired t test\n",
    "        first_i_values = first_i[name].values\n",
    "        second_df_i = second[second.author.isin(first_i.author.values)]\n",
    "        second_i_values = second_df_i[name].values\n",
    "    \n",
    "        assert first.shape[0] == second.shape[0]\n",
    "        assert first.author.tolist() == second.author.tolist() #make sure theyre alligned\n",
    "        \n",
    "        stat,p_value = wilcoxon(first_i_values, second_i_values)\n",
    "        p_values.append(p_value)\n",
    "    significance = ['*' if n<=0.05 else '' for n in p_values]\n",
    "    print(name, np.round(p_values,4))\n",
    "    plt.boxplot(values_across_comments, whis=(10,95))\n",
    "    maxes = [np.max(n) for n in values_across_comments]\n",
    "    max_value = np.max(maxes)\n",
    "    means = [np.mean(n) for n in values_across_comments]\n",
    "    means_markers = [\"^\" if n>0 else \"v\" for n in means]\n",
    "    plt.xlabel('Comments received')\n",
    "    labels = [str(n) for n in range(1,11)][:-1]+['10+']\n",
    "    plt.xticks(range(1,11), labels = labels)\n",
    "    plt.ylabel('Feature change\\n(Second - first post values)')\n",
    "    new_name = name.replace('_', ' ').capitalize()\n",
    "    plt.title(f'{new_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir+f'comments/delta_{name}.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
