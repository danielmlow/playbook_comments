{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download r/SuicideWatch Reddit submissions, comments, and prior and future submissions \n",
    "\n",
    "* Download all SW submissions\n",
    "* Take a random subsample\n",
    "* Find all other submissions from same users\n",
    "* Take a N months from SITBI submission to last submission\n",
    "* Label whether next submissions contain a SW submission or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7gCdEvrkTqVG",
    "outputId": "454b50f2-129a-4e45-8824-cc11d30b4812"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pprint import pprint\n",
    "import requests\n",
    "# from langdetect import detect\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import time\n",
    "import string\n",
    "from scipy import stats\n",
    "# import config #this is private and contains client info for Reddit pushshift API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "5G5H_MRzAIw-",
    "outputId": "8a58e0c6-683b-445a-89d1-bf473b61cb4f"
   },
   "outputs": [],
   "source": [
    "# # Run this cell to be able to mount GDrive and attach it to the colab so that we can save json outputs\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# # output dir if drive has been mounted:\n",
    "# output_dir = '/content/drive/My Drive/ML4HC_Final_Project/data/input/raw_submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "output_dir = './data/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftPNqH3DXsnV"
   },
   "outputs": [],
   "source": [
    "# Subreddits to scrape:\n",
    "\n",
    "subreddits = ['suicidewatch']\n",
    "\n",
    "metadata_to_keep_submissions = [\n",
    "    'id', #submission id                \n",
    "    'author',\n",
    "    'created_utc', #when submission was created\n",
    "    'subreddit', #subreddit\n",
    "    'title',\n",
    "    'selftext', #body\n",
    "    'score',\n",
    "    'num_comments',\n",
    "    'permalink', #need to add https://www.reddit.com\n",
    "]\n",
    "\n",
    "\n",
    "metadata_to_keep_comments =[\n",
    "    'id', \n",
    "    'author', \n",
    "    'created_utc', \n",
    "    'subreddit', \n",
    "    'body', #selftext\n",
    "    'score',\n",
    "    'is_submitter',\n",
    "    'link_id', \n",
    "    'parent_id',  \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4sVCDAJTwzt"
   },
   "outputs": [],
   "source": [
    "# Cell that contains helpful functions for datetime operations and parsing the json output from the url query\n",
    "import datetime as dt\n",
    "\n",
    "def gen_timestamp():\n",
    "    timestamp = '{:%Y-%m-%d-%H-%M-%S}'.format(dt.datetime.now())\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "\n",
    "def date2timestamp(date):\n",
    "    '''\n",
    "    \"01/12/2011\"\n",
    "    '''\n",
    "    return int(time.mktime(datetime.strptime(date, \"%Y/%m/%d\").timetuple()))\n",
    "\n",
    "def timestamp2date(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "\n",
    "def next_day(date):\n",
    "  tmrw = datetime.strptime(date, \"%Y/%m/%d\") + timedelta(days=1)\n",
    "  return tmrw.strftime('%Y/%m/%d')\n",
    "\n",
    "def list_of_days(start_date, end_date):\n",
    "    '''\n",
    "    start_date = date(2019, 1, 19)   # start date\n",
    "    end_date = date(2019, 3, 22)   # end date\n",
    "    '''\n",
    "    delta = end_date - start_date       # as timedelta\n",
    "    days = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        days.append(str(day).replace('-', '/'))\n",
    "    return days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_request(url):\n",
    "    request = requests.get(url,headers = {'User-agent': config.user_agent})\n",
    "    posts = request.json()\n",
    "    return posts\n",
    "\n",
    "def search_iteratively(submission_or_comment = 'submission',author_username = 'USERNAME', earliest_date=1119672000):\n",
    "    '''\n",
    "    Based on: https://github.com/bilsun/reddit-scraper\n",
    "    earliest_date = 1119672000 # 6/25/05 @ 12am | when Reddit was founded |https://www.unixtimestamp.com/index.php\n",
    "    sort = 'desc' #'asc' results in \"Too many requests\" error\n",
    "    \n",
    "    '''\n",
    "    sort = 'desc'\n",
    "    \n",
    "    \n",
    "    submission_fields = 'id,score,full_link,subreddit,title,selftext,created_utc,author,num_comments' \n",
    "    url = f\"https://api.pushshift.io/reddit/search/{submission_or_comment}/?author={author_username}&fields={submission_fields}&after={earliest_date}&size=1000&sort={sort}&metadata=true\"\n",
    "    \n",
    "    # One could add other attributes:\n",
    "    # keywords = 'bias|prejudice'\n",
    "    # subreddits = 'AskSocialScience,AskFeminists' \n",
    "    # url = f\"https://api.pushshift.io/reddit/search/submission/?q={keywords}&subreddit={subreddits}&fields={submission_fields}&after={earliest_date}&size=1000&sort=desc&metadata=true\"\n",
    "    \n",
    "\n",
    "    # paginating results (collect 1000 posts at a time to work around Pushshift's size limit)\n",
    "    start_from = ''\n",
    "    data = []\n",
    "    could_not_download = []\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            posts = make_request(url+start_from)\n",
    "    #         print(\"keywords: \" + keywords + \" | \" + str(posts['metadata']['total_results']) + \" posts found\")\n",
    "        except:\n",
    "            try: \n",
    "                n=0.5\n",
    "                time.sleep(n)\n",
    "                posts = make_request(url+start_from)\n",
    "                print(f'sleeping {n} sec')\n",
    "            except:\n",
    "                try: \n",
    "                    n = 1\n",
    "                    time.sleep(n)\n",
    "                    posts = make_request(url+start_from)\n",
    "                    print(f'sleeping {n} sec')\n",
    "                except:\n",
    "                    try:\n",
    "                        n = 5\n",
    "                        time.sleep(n)\n",
    "                        posts = make_request(url+start_from)\n",
    "                        print(f'sleeping {n} sec')\n",
    "                    except:\n",
    "                        print(f'-------could not download {url}')\n",
    "                        could_not_download.append(url)\n",
    "        # make sure Pushshift is gathering all Reddit data (IMPORTANT IF SCRAPING FOR RESEARCH)\n",
    "        total_posts = posts['metadata']['shards'][\"total\"]\n",
    "        assert(posts['metadata']['shards'][\"successful\"]==total_posts) \n",
    "        \n",
    "\n",
    "        data.extend(posts[\"data\"])\n",
    "        if len(posts[\"data\"]) == 0:\n",
    "                break # stop collecting data once there's nothing left to collect\n",
    "\n",
    "        last_utc = data[-1]['created_utc']\n",
    "        start_from = '&before=' + str(last_utc)\n",
    "\n",
    "#     print(\"successful data collection!\\n\")\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:    \n",
    "        df = df.sort_values(by='created_utc').reset_index(drop=True) #from oldest to newest\n",
    "    return df, could_not_download\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_json(url):\n",
    "    # parse request      \n",
    "    could_not_download = []\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        result = result.json()\n",
    "\n",
    "    except:\n",
    "        try: \n",
    "            n=0.5\n",
    "            time.sleep(n)\n",
    "            result = requests.get(url)\n",
    "            result = result.json()\n",
    "            print(f'sleeping {n} sec')\n",
    "        except:\n",
    "            try: \n",
    "                n=1\n",
    "                time.sleep(n)\n",
    "                result = requests.get(url)\n",
    "                result = result.json()\n",
    "                print(f'sleeping {n} sec')\n",
    "            except:\n",
    "                try: \n",
    "                    n=5\n",
    "                    time.sleep(n)\n",
    "                    result = requests.get(url)\n",
    "                    result = result.json()\n",
    "                    print(f'sleeping {n} sec')\n",
    "                except:\n",
    "                    print('failed!')\n",
    "                    could_not_download.append(url)\n",
    "                    return pd.DataFrame(could_not_download)\n",
    "    result = result['data']\n",
    "    df = pd.DataFrame(result)\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_ids(ids, submission_or_comment='comment'):\n",
    "    ids = ','.join(ids)\n",
    "    url = f'https://api.pushshift.io/reddit/search/{submission_or_comment}/?ids={ids}'\n",
    "    result = url_to_json(url)\n",
    "    result = pd.DataFrame(result)\n",
    "    result =result.loc[:,result.columns.isin(metadata_to_keep_comments)] # one of the comments didnt have certain cols\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_comments_by_sub_id(submission_id = ''):\n",
    "    #limited to 1000\n",
    "    comment_fields = 'id,author,created_utc,subreddit,body,score,is_submitter,link_id,parent_id'\n",
    "    url = f'https://api.pushshift.io/reddit/comment/search/?link_id={submission_id}&fields={comment_fields}&limit=1000'\n",
    "#     url = f'https://api.pushshift.io/reddit/comment/search/?link_id={submission_id}&limit=1000'\n",
    "    comments = url_to_json(url)\n",
    "    return comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_comments_iteratively(submission_id = '', earliest_date=1119672000):\n",
    "    '''\n",
    "    Based on: https://github.com/bilsun/reddit-scraper\n",
    "    earliest_date = 1119672000 # 6/25/05 @ 12am | when Reddit was founded |https://www.unixtimestamp.com/index.php\n",
    "    sort = 'desc' #'asc' results in \"Too many requests\" error\n",
    "    \n",
    "    '''\n",
    "    sort = 'desc'\n",
    "    submission_fields = 'id,score,full_link,subreddit,title,selftext,created_utc,author,num_comments' \n",
    "    url = f\"https://api.pushshift.io/reddit/search/comments/?link_id={submission_id}&fields={submission_fields}&after={earliest_date}&size=1000&sort={sort}&metadata=true\"\n",
    "    \n",
    "    # One could add other attributes:\n",
    "    # keywords = 'bias|prejudice'\n",
    "    # subreddits = 'AskSocialScience,AskFeminists' \n",
    "    # url = f\"https://api.pushshift.io/reddit/search/submission/?q={keywords}&subreddit={subreddits}&fields={submission_fields}&after={earliest_date}&size=1000&sort=desc&metadata=true\"\n",
    "    \n",
    "\n",
    "    # paginating results (collect 1000 posts at a time to work around Pushshift's size limit)\n",
    "    start_from = ''\n",
    "    data = []\n",
    "    could_not_download = []\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            posts = make_request(url+start_from)\n",
    "    #         print(\"keywords: \" + keywords + \" | \" + str(posts['metadata']['total_results']) + \" posts found\")\n",
    "        except:\n",
    "            try: \n",
    "                n=0.5\n",
    "                time.sleep(n)\n",
    "                posts = make_request(url+start_from)\n",
    "                print(f'sleeping {n} sec')\n",
    "            except:\n",
    "                try: \n",
    "                    n = 1\n",
    "                    time.sleep(n)\n",
    "                    posts = make_request(url+start_from)\n",
    "                    print(f'sleeping {n} sec')\n",
    "                except:\n",
    "                    try:\n",
    "                        n = 5\n",
    "                        time.sleep(n)\n",
    "                        posts = make_request(url+start_from)\n",
    "                        print(f'sleeping {n} sec')\n",
    "                    except:\n",
    "                        print(f'-------could not download {url}')\n",
    "                        could_not_download.append(url)\n",
    "        # make sure Pushshift is gathering all Reddit data (IMPORTANT IF SCRAPING FOR RESEARCH)\n",
    "        total_posts = posts['metadata']['shards'][\"total\"]\n",
    "        assert(posts['metadata']['shards'][\"successful\"]==total_posts) \n",
    "        \n",
    "\n",
    "        data.extend(posts[\"data\"])\n",
    "        if len(posts[\"data\"]) == 0:\n",
    "                break # stop collecting data once there's nothing left to collect\n",
    "\n",
    "        last_utc = data[-1]['created_utc']\n",
    "        start_from = '&before=' + str(last_utc)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:    \n",
    "        df = df.sort_values(by='created_utc').reset_index(drop=True) #from oldest to newest\n",
    "    return df, could_not_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kds2-QaRWL0x"
   },
   "outputs": [],
   "source": [
    "# # Method used to get submissions\n",
    "# def scrape_reddit(output_dir, subreddit, date_start, date_end, size = 1000):\n",
    "#     '''\n",
    "#     size = {1,1000} #amount of submissions\n",
    "#     '''\n",
    "#     start = date2timestamp(date_start)\n",
    "#     end = date2timestamp(date_end)\n",
    "#     # use the pushshift api to extract out data\n",
    "#     url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&sort=desc&sort_type=created_utc&after={start}&before={end}&size={size}'\n",
    "# #     print(url)\n",
    "#     try:\n",
    "#         submissions = requests.get(url)\n",
    "#         submissions = submissions.json()\n",
    "#         submissions = submissions['data']\n",
    "#     except:\n",
    "#         print('sleeping for 1 seconds...')\n",
    "#         time.sleep(1)\n",
    "#         submissions = requests.get(url)\n",
    "#         submissions = submissions.json()\n",
    "#         submissions = submissions['data']\n",
    "\n",
    "#     df = pd.DataFrame(columns=['subreddit', 'author', 'date', 'submission', 'num_comments', 'score'])\n",
    "    \n",
    "#     for submission in submissions:\n",
    "#         if 'selftext' in submission: # check if selftext parameter exists\n",
    "#             text = submission['selftext']\n",
    "#             if text != \"\" and  text != '[removed]' and '[deleted]' not in text: # further check if selftext is not empty\n",
    "#                 try: \n",
    "#                     if detect(text) == 'en': # check if text is in english - if the language detected is not in the langdetect library, then continue to the next submission\n",
    "#                         df = df.append({'subreddit': subreddit, \n",
    "#                                   'author': submission['author'], \n",
    "#                                   'date': date_start, \n",
    "#                                   'submission': submission['title'] + ' ' + submission['selftext'],\n",
    "#                                   'num_comments': submission['num_comments'], \n",
    "#                                   'score': submission['score'],\n",
    "#                                  }, ignore_index=True)\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all submissions\n",
    "\n",
    "From Reddit Mental Health Dataset (Low et al. (2020). JMIR), we obtained SuicideWatch post's usernames. Then we searched for all their prior and subsequent submissions. We obtained the first SW submission, and obtained the comments for that submission. We also labelled (a) 1 if subsquent subreddits contained SW and (b) whether there were any subreddits contained mental health or other support-seeking subreddits (personalfinance?)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain SW submissions from Reddit Mental Health Dataset (Low et al, 2020. JMIR)\n",
    "\n",
    "sw2018 = pd.read_csv(input_dir+'suicidewatch_2018_features_tfidf_256.csv')\n",
    "sw2019 = pd.read_csv(input_dir+'suicidewatch_2019_features_tfidf_256.csv')\n",
    "sw = pd.concat([sw2018,sw2019], axis=0)\n",
    "sw = sw.reset_index(drop=True)\n",
    "sw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = sw.author.unique()\n",
    "print(len(authors), 'unique authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False \n",
    "\n",
    "if run:\n",
    "    all_submissions = []\n",
    "    print('total: ', len(authors))\n",
    "    could_not_download_all = []\n",
    "    for i, author in enumerate(authors):\n",
    "        if i%50==0:\n",
    "            print(i)\n",
    "        try: \n",
    "            all_submissions_i, could_not_download = search_iteratively(submission_or_comment = 'submission',author_username = author, earliest_date=1119672000)\n",
    "            could_not_download_all.append(could_not_download)\n",
    "            all_submissions.append(all_submissions_i)\n",
    "        except:\n",
    "            print(f'could not download {author}')\n",
    "            could_not_download_all.append(author)\n",
    "        if i%1000==0:\n",
    "            # save every 1000 users in case it fails             \n",
    "            posts_per_user = [n.shape[0] for n in all_submissions]    \n",
    "            all_submissions = pd.concat(all_submissions)\n",
    "            pd.DataFrame(all_submissions).to_csv(input_dir+f'sw_users_all_submissions_{gen_timestamp()}_{i}.csv')        \n",
    "            could_not_download_all = [n for n in could_not_download_all if len(n)==0]\n",
    "            pd.DataFrame(could_not_download_all).to_csv(input_dir+f'sw_users_all_submissions_{gen_timestamp()}_{i}_could-not-download.csv')\n",
    "            all_submissions = []\n",
    "            could_not_download_all = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile submissions dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(input_dir)\n",
    "files = [n for n in files if 'sw_users_all_submissions' in n]\n",
    "could_not_download = [n for n in files if 'could_not_download' in n]\n",
    "files =  [n for n in files if 'could_not_download' not in n]\n",
    "files.sort()\n",
    "\n",
    "all_submissions = []\n",
    "for file in files:\n",
    "    submissions_i = pd.read_csv(input_dir+file, index_col=0)\n",
    "    all_submissions.append(submissions_i)\n",
    "\n",
    "    \n",
    "all_submissions = pd.concat(all_submissions)\n",
    "all_submissions = all_submissions.reset_index(drop=True)\n",
    "\n",
    "all_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1307323 rows\n",
    "\n",
    "\n",
    "885889 rows without nans `all_submissions.dropna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = all_submissions.dropna().shape[0]\n",
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions.dropna().to_csv(input_dir+f'all_subs_{gen_timestamp()}.csv') # all_subs_2021-06-08-08-20-26.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = pd.read_csv(input_dir+'all_subs_2021-06-08-08-20-26.csv', index_col = 0)\n",
    "\n",
    "all_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "posts_per_user = Counter(all_submissions.author.values)\n",
    "posts_per_user = pd.DataFrame(posts_per_user, index = ['counts']).T\n",
    "posts_per_user = posts_per_user.sort_values(by='counts')[::-1]\n",
    "posts_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_per_user_df = posts_per_user.value_counts().reset_index().sort_values('counts')\n",
    "posts_per_user_df.columns = ['posts','users']\n",
    "posts_per_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_per_user.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.displot(posts_per_user, x='counts', discrete=True)# , bins=posts_per_user.shape[0])\n",
    "# plt.bar(x=posts_per_user_df.posts,height= posts_per_user_df.users, align='edge')# , bins=posts_per_user.shape[0])\n",
    "plt.xlabel('Posts')\n",
    "plt.ylabel('Users')\n",
    "plt.xlim(0,200)\n",
    "plt.show()\n",
    "# plt.hist(posts_per_user, bins=200)\n",
    "# plt.xlabel('log(Posts per user)')\n",
    "# plt.xlim(0,600)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posts_per_user.describe().astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find first SW post, and only get comments for that post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1650105 - 1642819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_submissions_sw = all_submissions[all_submissions.subreddit=='SuicideWatch']\n",
    "all_submissions_sw = all_submissions_sw.sort_values(by=['author','created_utc'])\n",
    "all_submissions_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST confirm date is oldest to newest: PASSED\n",
    "[print(timestamp2date(int(n))) for n in all_submissions_sw[all_submissions_sw.author =='--dark--phoenix--'].created_utc.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Test: Make sure times are properly sorted: PASSED\n",
    "# for author in all_submissions_sw.author.unique()[:100]:\n",
    "#     df_author = all_submissions_sw[all_submissions_sw['author']== author]\n",
    "#     print('\\n\\n')\n",
    "#     [print(n) for n in df_author.full_link.values[:5]]\n",
    "#     [print(datetime.datetime.fromtimestamp(n, tz=timezone.utc)) for n in df_author.created_utc.values[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions_sw_first = all_submissions_sw.drop_duplicates(subset=['author'],keep='first')\n",
    "all_submissions_sw_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first SW post\n",
    "all_submissions_sw_first = all_submissions_sw.drop_duplicates(subset=['author'],keep='first')\n",
    "all_submissions_sw_first.to_csv(input_dir+f'first_sw_posts_{gen_timestamp()}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions_sw_first = pd.read_csv(input_dir+'first_sw_posts_2021-07-18-14-58-03.csv', index_col = 0)\n",
    "all_submissions_sw_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There aren't post that have more than 250, so I can use simple method for searching for comments\n",
    "[print(n) for n in all_submissions_sw_first[all_submissions_sw_first.num_comments>250].full_link.values]\n",
    "# '82i8oz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions_sw_first['id'][all_submissions_sw_first['num_comments']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw_post_ids = all_submissions_sw_first[all_submissions_sw_first.num_comments>0]['id'].values #only obtain for those >0 comments\n",
    "\n",
    "try: os.mkdir(input_dir+'comments_i/')\n",
    "except: pass\n",
    "\n",
    "all_comments = []\n",
    "print('total: ', len(first_sw_post_ids))\n",
    "\n",
    "restart = 15000\n",
    "for i, post_id in enumerate(first_sw_post_ids[restart:]):\n",
    "    i+=restart\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    \n",
    "    comments = search_comments_by_sub_id(submission_id = post_id) # search for comments\n",
    "    all_comments.append(comments) \n",
    "    \n",
    "    if i%1000==0 and i!=restart:\n",
    "        all_comments_df = pd.concat(all_comments)        \n",
    "        all_comments_df.to_csv(input_dir+f'comments_i/first_sw_comments_{gen_timestamp()}_{i}.csv')\n",
    "        all_comments = []\n",
    "\n",
    "# do for last few\n",
    "if len(all_comments) > 0:\n",
    "    all_comments_df = pd.concat(all_comments)        \n",
    "    all_comments_df.to_csv(input_dir+f'comments_i/first_sw_comments_{gen_timestamp()}_{i}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments_df = pd.concat(all_comments)        \n",
    "# all_comments_df.to_csv(input_dir+f'comments_i/first_sw_comments_{gen_timestamp()}_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = search_comments_by_sub_id(submission_id = post_id) # search for comments\n",
    "# comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile into one df\n",
    "comments_dir= input_dir+'comments_i/'\n",
    "files = os.listdir(comments_dir)\n",
    "all_comments = []\n",
    "for file in files:\n",
    "    df_i = pd.read_csv(comments_dir+file, index_col = 0)\n",
    "    all_comments.append(df_i)\n",
    "\n",
    "all_comments_df = pd.concat(all_comments)        \n",
    "all_comments_df.reset_index(drop=True).to_csv(input_dir+f'first_sw_comments_{gen_timestamp()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = pd.read_csv(input_dir+'first_sw_comments_{}.csv', index_col=0)\n",
    "all_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTs\n",
    "assert all_comments[all_comments.subreddit!='SuicideWatch'].shape[0]==0\n",
    "# all_comments.link_id.unique().shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove deleted and removed posts AND DUPLICATES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "subs = pd.read_csv(input_dir+'all_subs_2021-06-08-08-20-26.csv', index_col=0)\n",
    "first_sw = pd.read_csv(input_dir+'first_sw_posts_2021-07-18-14-58-03.csv', index_col=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(input_dir+'first_sw_comments_2021-07-20-13-01-41.csv', index_col=0)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The author of the comment removed their comment then tis consider “[deleted]” . If the comment was offensive to the mods or broken the rules of the sub-Reddit then it removed, hence “[removed]”.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "authors_to_remove = []\n",
    "comments_to_remove = []\n",
    "posts_to_remove = []\n",
    "\n",
    "\n",
    "\n",
    "dfs = [subs,first_sw, comments]\n",
    "names = ['subs','first_sw', 'comments']\n",
    "\n",
    "for df_i, name in zip(dfs, names):\n",
    "    remove_keys = ['[deleted]', '[removed]']\n",
    "    if name == 'comments':\n",
    "        check_cols = ['author', 'body']\n",
    "    else:\n",
    "        check_cols = ['author', 'title', 'selftext']\n",
    "        \n",
    "    for i in remove_keys:\n",
    "        for col in check_cols:\n",
    "            print(name, col, i, df_i[(df_i[col]==i) | (df_i[col].astype(str).str.startswith(i)) ].shape[0], f'/{df_i.shape[0]}')\n",
    "\n",
    "#             if name == 'comments':\n",
    "#                 comments_to_remove.append(df_i[df_i[col]==i])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[comments.author.astype(str).str.startswith('[deleted] ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[comments.body.astype(str).str.startswith('[deleted] ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# These posts were removed by moderators. Since we use text for analysis, we cannot analyze these\n",
    "remove_authors = first_sw[first_sw['selftext']=='[removed]'].author.values\n",
    "remove_comments = first_sw[first_sw['author'].isin(remove_authors)].id.values\n",
    "remove_comments = np.array(['t3_'+n for n in remove_comments])\n",
    "print(remove_authors[:10])\n",
    "print(remove_authors.shape)\n",
    "print(remove_comments.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[subs['selftext'].astype(str).str.startswith('[deleted]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST all start with t3: PASSED\n",
    "print(comments.link_id.values.shape)\n",
    "print(comments.link_id.str.startswith('t3_').values.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_comments\n",
    "print(comments.shape) \n",
    "comments = comments[~comments.link_id.isin(remove_comments)] #because some start with t3_ or tN\n",
    "print(comments.shape) #around 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(input_dir+f'first_sw_comments_{gen_timestamp()}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST, should be around median of 4 comments per user: PASSED\n",
    "(103042-102414)/157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = [subs,first_sw]\n",
    "names = ['all_submissions','first_sw_submission']\n",
    "\n",
    "for df_i, name in zip(dfs, names):\n",
    "    remove_keys = ['[deleted]']\n",
    "    check_cols = ['author', 'title', 'selftext']\n",
    "    print('\\n\\n=======',name)\n",
    "    print(df_i.shape)\n",
    "    df_i = df_i[~df_i.author.isin(remove_authors)]\n",
    "    print('removed authors')\n",
    "    print(df_i.shape)\n",
    "    if name != 'first_sw_comments':\n",
    "        for i in remove_keys:\n",
    "            for col in check_cols:\n",
    "                print(i, col)\n",
    "\n",
    "                df_i =   df_i[~((df_i[col]==i) | (df_i[col].astype(str).str.startswith(i)) )]\n",
    "                print(df_i.shape)\n",
    "    df_i.to_csv(input_dir+f'{name}_{gen_timestamp()}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since username and author are removed from both pushshift and Reddit (accessed through permalink/full_link), we keep all comments. Having comments that violate subreddit rules or are offensive can be informative (even if the text is not available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile into single DF\n",
    "files = os.listdir(input_dir+'submissions_old/')\n",
    "\n",
    "all_subs = [all_submissions_sw]\n",
    "files.sort()\n",
    "for file in files:\n",
    "    df_i = pd.read_csv(f'{input_dir}submissions_old/{file}', index_col = 0)\n",
    "    all_subs.append(df_i)\n",
    "\n",
    "\n",
    "all_subs = pd.concat(all_subs)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n",
    "all_subs.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = all_subs.reset_index(drop=True)\n",
    "all_subs = all_subs.drop('0', axis=1)\n",
    "\n",
    "# categorical\n",
    "cols = ['author','subreddit']\n",
    "for col in cols:\n",
    "    all_subs[col] = all_subs[col].astype(\"category\")\n",
    "\n",
    "all_subs.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_subs.to_csv(input_dir+'all_subs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob want to remove these or try to redownload with id\n",
    "\n",
    "necessary_cols = ['author', 'created_utc', 'id', 'subreddit']\n",
    "df = all_subs[necessary_cols]\n",
    "\n",
    "all_subs[~df.isnull().any(axis=1)] #all minus those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical\n",
    "cols = ['num_comments','score']\n",
    "for col in cols:\n",
    "    all_subs[col] = all_subs[col].astype(\"int\")\n",
    "\n",
    "all_subs.memory_usage(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instead of using Reddit Mental Health Dataset, you can download data yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start = date2timestamp(date_start)\n",
    "# end = date2timestamp(date_end)\n",
    "# url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&sort=desc&sort_type=created_utc&after={start}&before={end}&size={size}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "89SpYbPpX8PU",
    "outputId": "a14c4947-b3fd-4421-a2ad-e009102f6ad6"
   },
   "outputs": [],
   "source": [
    "# # Extract out reddit data given specifications of subreddits + pre pandemic distinction, as well as window of days to extract from\n",
    "\n",
    "\n",
    "# for year in range(2017,2019): #excluding 2020 due to Covid-19\n",
    "#     print(year,'=======================')\n",
    "#     days = list_of_days(date(year,1,1), date(year,12,31))\n",
    "#     days_local = list(days)\n",
    "#     size = 30 #1000 is the max. amount of submissions you can download per request.\n",
    "#     size_of_subreddit = 3000 #so with size=100 and size_of_subreddit=10000 you'll get at least 100 days (10000/100) or more per year\n",
    "\n",
    "#     for subreddit in subreddits:\n",
    "#         print(subreddit)\n",
    "#         subreddit_df = pd.DataFrame(columns=['subreddit', 'author', 'date', 'submission']) #empty df\n",
    "#         while subreddit_df.shape[0] < size_of_subreddit and days_local:\n",
    "#             # download submissions from a random day until you reach size_of_subreddit. some days will have less than size.         \n",
    "#             idx = random.randint(0, len(days_local)-1)\n",
    "#             date_start = days_local.pop(idx) #pop guaranteees it won't repeat days\n",
    "#             date_end = next_day(date_start)\n",
    "#             df = scrape_reddit(output_dir, subreddit, date_start, date_end, size = size)\n",
    "#             subreddit_df = pd.concat([subreddit_df, df])\n",
    "#             time.sleep(0.1)\n",
    "# #             print(subreddit)\n",
    "#             print(subreddit_df.shape)\n",
    "#         subreddit_df.to_csv(os.path.join(output_dir, f'{subreddit}_{year}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ruk9QhfWqzTZ",
    "outputId": "1b9fe891-d50d-4ccf-8110-3c53bcebd3e1"
   },
   "outputs": [],
   "source": [
    "# # Extract out reddit data given specifications of subreddits from 2018, as well as window of days to extract from\n",
    "\n",
    "# days = list_of_days(date(2018,1,1), date(2018,12,31))\n",
    "# size = 1000\n",
    "\n",
    "# timeframe = '2018'\n",
    "\n",
    "# for subreddit in subreddits[3:]:\n",
    "#   subreddit_df = pd.DataFrame(columns=['subreddit', 'author', 'date', 'submission'])\n",
    "#   days_local = list(days)\n",
    "#   while subreddit_df.shape[0] < 30000 and days_local:\n",
    "#     idx = random.randint(0, len(days_local)-1)\n",
    "#     date_start = days_local.pop(idx)\n",
    "#     date_end = next_day(date_start)\n",
    "#     df = scrape_reddit(output_dir, subreddit, timeframe, date_start, date_end, size = size)\n",
    "#     subreddit_df = pd.concat([subreddit_df, df])\n",
    "#     time.sleep(0.5)\n",
    "#     print(subreddit)\n",
    "#     print(subreddit_df.shape)\n",
    "#   subreddit_df.to_csv(os.path.join(output_dir, '{}_{}.csv'.format(subreddit, timeframe)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1YU-7sS1VOU5",
    "outputId": "0cad5263-5243-46af-d2df-3e334a918d6e"
   },
   "outputs": [],
   "source": [
    "# # Extract out reddit data given specifications of subreddits + submission pandemic distinction, as well as window of days to extract from\n",
    "\n",
    "# days = list_of_days(date(2018,1,1), date(2018,4,20))\n",
    "# size = 1000\n",
    "\n",
    "# timeframe = '2018'\n",
    "\n",
    "# for subreddit in subreddits:\n",
    "#   subreddit_df = pd.DataFrame(columns=['subreddit', 'author', 'date', 'submission'])\n",
    "#   days_local = list(days)\n",
    "#   for date_start in days_local:\n",
    "#     date_end = next_day(date_start)\n",
    "#     df = scrape_reddit(output_dir, subreddit, timeframe, date_start, date_end, size = size)\n",
    "#     subreddit_df = pd.concat([subreddit_df, df])\n",
    "#     time.sleep(0.5)\n",
    "#     print(subreddit)\n",
    "#     print(subreddit_df.shape)\n",
    "#   subreddit_df.to_csv(os.path.join(output_dir, '{}_{}.csv'.format(subreddit, timeframe)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "reddit_data_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
