{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "* Takes post df and comments df from download_reddit_posts_and_comments.ipynb and finds first suicidal post, counts distribution of subsequent posts, and adds comments\n",
    "* Feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installed in requirements, if not install here.\n",
    "# !pip install nltk\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "output_dir = './data/output/psm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "def unixtimestamp_to_readable(unix_timestamp=1284101485):\n",
    "    \n",
    "    ts = int(unix_timestamp)\n",
    "\n",
    "    # if you encounter a \"year is out of range\" error the timestamp\n",
    "    # may be in milliseconds, try `ts /= 1000` in that case\n",
    "    return datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "def unixtimestamp_to_datetime(unix_timestamp=1284101485):\n",
    "    unix_timestamp = int(unix_timestamp)\n",
    "    dt_object = dt.datetime.fromtimestamp(unix_timestamp)\n",
    "    return dt_object\n",
    "\n",
    "def datetime_to_unixtimestamp(datetime_obj):\n",
    "    timestamp = datetime.timestamp(datetime_obj)\n",
    "    return int(timestamp)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "#     text = text.strip()\n",
    "    text = re.sub('&amp;', '&', text)\n",
    "    text = re.sub('&lt;', '<', text)\n",
    "    text = re.sub('&gt;', '>', text)\n",
    "    text = re.sub('&#x200B;', '', text)\n",
    "    text = re.sub('&nbsp;', ' ', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all posts and comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts\n",
    "\n",
    "df = pd.read_csv(input_dir+'first_sw_submission_2021-07-20-23-12-27.csv',index_col=0, lineterminator='\\n')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_all = []\n",
    "for title, body in zip(df['title'].values, df['selftext'].values):\n",
    "    title = title.strip()\n",
    "    if title.endswith('.') or title.endswith('?') or title.endswith('!'):\n",
    "        title += ' '\n",
    "    else:\n",
    "        title += '. '\n",
    "    text = title+str(body).strip()\n",
    "    text_all.append(text)\n",
    "\n",
    "text_all = pd.DataFrame(text_all)\n",
    "text_all\n",
    "        \n",
    "\n",
    "df['text']  = text_all\n",
    "        \n",
    "df[['text', 'title', 'selftext']]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor = '\\n+', '&amp;','&lt;','&gt;','&#x200B;','&nbsp;'\n",
    "errors = df[df.text.str.contains('|'.join(searchfor))]\n",
    "print(errors.text.tolist()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text'] = [clean_text(n) for n in df.text.values]\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(input_dir+'first_sw_submission_2021-07-20-23-12-27.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually run through LIWC2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(input_dir+'first_sw_submission_2021-07-20-23-12-27.csv', index_col=0, lineterminator='\\n')\n",
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_dir = './data/input/'\n",
    "# Rename LIWC columns\n",
    "df_original = pd.read_csv(input_dir+'first_sw_submission_2021-07-20-23-12-27.csv', index_col=0, lineterminator='\\n')\n",
    "df_cols  = df_original.columns.tolist()\n",
    "df = pd.read_csv(input_dir+'first_sw_submission_liwc2015_2021-07-20-23-12-27.csv', index_col=0)\n",
    "df.columns = df_cols+df.columns.tolist()[len(df_cols):]\n",
    "df.to_csv(input_dir+'first_sw_submission_liwc2015_2021-07-20-23-12-27.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "# Rename LIWC columns\n",
    "df_sub = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_2021-07-20-23-12-27.csv', index_col=0)\n",
    "liwc = pd.read_csv(input_dir+'first_sw_submission_liwc2015_2021-07-20-23-12-27.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_sub.id.values)- set(liwc.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = liwc[liwc.id != '81ust5']\n",
    "liwc = liwc[liwc.id != '7xm567']\n",
    "df_sub = df_sub[df_sub.id != '81ust5']\n",
    "df_sub = df_sub[df_sub.id != '7xm567']\n",
    "\n",
    "\n",
    "\n",
    "liwc = liwc.dropna(subset=['id'])\n",
    "df_sub =df_sub.dropna(subset=['id'])\n",
    "liwc = liwc.sort_values('author')\n",
    "df_sub = df_sub.sort_values('author')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(df_sub.id.values == liwc.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sub.shape)\n",
    "print(liwc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = list(set(liwc.columns).intersection(set(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = liwc.merge(df_sub, on=common_columns)\n",
    "df.to_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_2021-07-20-23-12-27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_original = pd.read_csv(input_dir+'first_sw_comments_2021-07-20-18-52-35.csv', index_col=0)\n",
    "comments_original['body'] = [clean_text(n) for n in comments_original.body.values]\n",
    "comments_original.to_csv(input_dir+'first_sw_comments_2021-07-20-18-52-35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform through LIWC2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_original = pd.read_csv(input_dir+'first_sw_comments_2021-07-20-18-52-35.csv', index_col=0)\n",
    "comments_cols  = comments_original.columns.tolist()\n",
    "comments = pd.read_csv(input_dir+'first_sw_comments_liwc2015_2021-07-20-18-52-35.csv', index_col=0)\n",
    "comments.columns = comments_cols+comments.columns.tolist()[len(comments_cols):]\n",
    "comments = comments.drop('permalink', axis=1)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments.to_csv(input_dir+'first_sw_comments_liwc2015_2021-07-20-18-52-35.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: passed\n",
    "# Check that permalinks are correct. I checked. They're correct.\n",
    "# print(df.title.values[100])\n",
    "# print(df.full_link.values[100])\n",
    "# print(df.title.values[20000])\n",
    "# print(df.full_link.values[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST: passed\n",
    "# # check that for a given post, I have the comments. \n",
    "# post_i = 'ar8ydo'\n",
    "# comments_i = comments[comments.link_id.str.contains(post_i)]\n",
    "# print('www.reddit.com/'+comments_i.permalink.values[0])\n",
    "\n",
    "# post_row = df[df.id==post_i]\n",
    "# content = post_row.title+'\\n'+post_row.selftext\n",
    "# print(content.values[0])\n",
    "# comments_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write in lowercase, only punctuation are apostrophes like \"i'm\", mainly oriented to first person singular\n",
    "# lexicons may assume the document contains complaints and stressors. So it counts 'dad' as domestic_stress_and_violence.\n",
    "# word stems are included were possible to capture different suffixes\n",
    "\n",
    "financial_stress = [\n",
    "'credit card', 'bank', \"bad credit\",\"my credit\",'loan', 'lend',\n",
    "'econom',\"financ\",\n",
    "'rent', 'mortgage', 'evict','insurance',\n",
    "\"money\",\n",
    "'owe', 'debt', \"make ends meet\",\"make ends meat\",\n",
    "'pay off','payment', 'taxes',\n",
    "'bills','tuition','lose my scholarship',\n",
    "\"afford\",\n",
    "\"save enough money\", \"save money\",\"saving\",\n",
    "'salary', 'wage', 'income', 'job','employ', #unemploy\n",
    "'foreclosure', 'bankrupt',\n",
    "'poverty', \"poor\", 'disadvantaged']\n",
    "\n",
    "domestic_stress_and_violence = ['divorc', 'marri','marry','widow','violence', 'abuse', 'yelling','screaming',\n",
    "'cheat','ghost','selfish','self centered',\n",
    "'boyfriend', 'girlfriend', 'partner', 'husband', 'wife','dad', 'mom', 'brother', 'sister', 'sibling',\n",
    "\"we're fighting\", 'single mom', 'single dad', 'single parent', 'hit me',\n",
    "'slapped me','kick','punch', 'fight','fought', 'aggressi', 'agresi', 'agressi', 'choke','slam', 'push me', 'pushed me',\n",
    "'forced me to', 'knocked me', 'strike me', 'cut me', 'threat', 'stab', 'control me', 'controlling me',\n",
    "'knife', 'knives','stab', 'blade', 'gouge', 'dagger','gun', 'pistol', 'revolver', 'semiautomatic', 'semi automatic','rifle', 'shoot', 'firearm', ]\n",
    "\n",
    "loneliness_isolation = ['alone', 'lonely', 'no one cares about me', 'no one cares', \"can't see anyone\", 'no one else to talk to',\n",
    "\"can't see my\", \"i miss my\", \"i want to see my\", \"trapped\", \"i'm in a cage\",\n",
    "\"feel ignored\", \"ignoring me\" \"ugly\", 'rejected', 'avoiding me', \" lack of social\",'akward',\n",
    "\"no friends\", \"any friends\",\"make friends\", \"want someone to\",\"want anyone to\", \"have no one\", \"to hang out\",\n",
    "\"people in my life\",\"wrong with me\",\n",
    "'am single', 'been single', 'quarantine', 'lockdown', 'isolation', 'self-isolation', 'by myself',\n",
    "\"find someone\"]\n",
    "\n",
    "\n",
    "substance_use = [\n",
    "'substance','drug',\n",
    "'addict', 'clean', 'rehab', 'sober', 'relaps','withdraw','12 step',\n",
    "    'adderal', 'vyvans',\n",
    "'medicat','lexapro','prozac','zoloft','valium','xanax',\n",
    "'aa meeting', 'na meeting',\n",
    "'smoke','vape', 'inject', 'snort','needle',\n",
    "'drink', 'drunk', 'drank',  'overdos','oding','hung over', 'hangover',\n",
    "'cigarette', 'tobacco','nicotine','weed','cannabis', 'marihuana','ganja','hashish',\n",
    "'alcohol','beer',\t'vodka', 'whiskey', 'whisky', 'tequila','wine',\n",
    "'pill','capsule','barbiturate', 'pentobarbital', 'nembutal', 'zann','benzodiazepines', 'alprazolam', \n",
    "'chlorodiazepoxide', 'librium', 'diazepam', \n",
    "    'lorazepam', 'ativan', 'triazolam', 'halicon'\n",
    "'eszopiclone', 'lunesta','zaleplon', 'sonata','zolpidem','ambien',\n",
    "'opioid','cocaine','blow','crack',\n",
    "'heroine', 'smack','methadone','buprenorphine', 'naltrexone',\n",
    "'laughing gas', 'nitrite', 'paint thiner', 'nitrous oxide',\n",
    "'ketamine','special k',\n",
    "'khat','kratom',\n",
    "'shrooms','psilocybin', 'mushrooms', 'lsd', 'acid',\n",
    "'mescaline', 'peyote',\n",
    "'dxm','loperamide',\n",
    "'mdma', 'molly', 'ecstasy','dmt',\n",
    "'ghb', 'xyrem',\n",
    "'codeine', 'vicodin', 'fentanyl','codone','percocet', 'oxycontin',\n",
    "'adderall', 'methylphenidate', 'concerta', 'ritalin',\n",
    "'roofie', 'rohypnol','ruffie',\n",
    "'testosterone',\n",
    "'bath salt',\n",
    "'amphetamine', 'pcp','angel dust', 'stimulant',\n",
    "'inhalant', 'desoxyn', 'crank', 'chalk', 'gak',  'pookie', 'quartz', 'rocket fuel', 'scooby snax', 'took speed','taking speed', 'take speed' #to dissambiguate with normal speed\n",
    "'meth']\n",
    "\n",
    "suicidality_passive = [\n",
    "#    passive\n",
    "\"don't know how much longer I\",\n",
    "\"would be better off dead\", \"would be better off without me\",\"would be better off if I died\", #RISK FALSE POSITIVES\n",
    "\"want to be here anymore\",\"wanna to be here anymore\",\n",
    "\"want to die\", \"wanting to die\",\"wanted to die\",\"wanna die\",\n",
    "\"don't want to live\", 'not wanting to live',\n",
    "'want to go to sleep',\"wanna go to sleep\"\n",
    "'wish i could go to sleep',\n",
    "'close my eyes and never open them again',\n",
    "'not wake up',\n",
    "\"don't wake up\",\n",
    "'never wake up',\n",
    "'never want to wake up','never wanna wake up',\n",
    "\"don't want to wake up\",\"don't wanna wake up\",\n",
    "'wish it would all end', 'done with living', 'want it to end',\n",
    "'live anymore', 'living anymore', 'life anymore', 'be dead',\n",
    "\"nothing to live for\",\"nothing left to live for\",\n",
    "'shoot me', 'kill me',\"shot me\",\"killed me\", \n",
    "\"don't want to be alive\",\"don't wanna be alive\",\n",
    "'think about death','thinking about death',\n",
    "\"life isn't worth living\",\"life is not worth living\",\n",
    "\"i'm ready to go\", 'i am ready to go',\n",
    "'if I live or die', 'I hate my life','if I lived or die', 'I hated my life',\n",
    "'no point in living',\n",
    "    \"wish i were never born\",\n",
    "    \"have no future\",   \n",
    "        'my last day',\n",
    "        'let me die',\n",
    "    'let me go',\n",
    "]\n",
    "\n",
    "suicidality_general = [\n",
    "'suicid',\n",
    "'depress', #depressed, 'depression'\n",
    "    \"shitty life\",\n",
    "    \"sick of life\",\n",
    "    \"i deserve to die\",\n",
    "\n",
    "\n",
    "    \"life is worse\",\n",
    "    \"life is over\",\n",
    "    'life is shit',\n",
    "    'not be here',\n",
    "    \"when I'm not around\",\n",
    "\n",
    "    'crisis center', 'crisis hotline', 'crisis text',\n",
    "'thing keeping me','only thing that kept me', 'only thing stopping me', #going, alive, from X\n",
    "'thing that keeps me going','thing that keeps me alive','thing that keeps me from',\n",
    "\"want to do this anymore\",\"wanted to do this anymore\",\"wanna to do this anymore\",\n",
    "\"don't know how much longer\",\n",
    "\n",
    "    \"i'm worthless\",\n",
    "\n",
    "    \"i'm on the ledge\",\n",
    "\n",
    "    \n",
    "    \"say goodbye\",'goodbye world',\n",
    "    'hate myself',\n",
    "    'hate my life',\n",
    "    \"one choice\",\n",
    "    \n",
    "    \n",
    "    'want to end this',\n",
    "    \n",
    "    \n",
    "    # social: burdemness/loneliness\n",
    "\"i'm a burden\", \"i'm such a burden\",\n",
    "\"no one will miss me\",\n",
    "\"better off without me\",\n",
    "    \"wasting space\",\n",
    "    \"nobody wants me here\",\n",
    "# helplessness\n",
    "    'have no hope', 'having no hope',\n",
    "    \"it's too late\",\n",
    "'hopeless','helpless',\n",
    "\"don't know what to do\",\"don know what to do\",\"dont know what to do\",\"idk what to do\", \"not knowing what to do\",\n",
    "\"don't see the light at the end\", \"not seeing the light at the end\",\n",
    "\"do with my life\", \"done with my life\", #don't know what to do with my life,don't know what I want to do with my life,\n",
    "'take it anymore',\n",
    "\"point of living\",\n",
    "    \"there no point\",\"there's no point\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "suicidality_selfharm = ['self harm', 'selfharm',\n",
    "'self injur','cut my','slit', 'wrist cut','lacerat',\n",
    "'made cuts on my', 'made a cut on my', 'made a gash', 'gashed my', 'split open my',\n",
    "'pierced my', 'piercing my', 'pierce my','burn my', \n",
    "'cutting',\n",
    "'hurt myself', 'hurting my','hurt myself',\n",
    "]\n",
    "\n",
    "# active\n",
    "suicidality_active = [\n",
    "'commit suicide','commiting suicide',\n",
    "'attempt' #HIGH RISK FOR FALSE POSITIVES\n",
    "'jump off a bridge','jumping off a bridge',\n",
    "'throw myself into', #traffic\n",
    "'drive into something',\n",
    "'kill myself','killing myself','killed myself','kill yourself',\n",
    "'hang myself', 'hanging my','hung myself',\n",
    "'shoot myself', 'shooting myself',\n",
    "'gun in my mouth',\n",
    "'overdose','overdosing','oding', 'offing',\n",
    "'it all ends tonight',\n",
    "'end my life','ending my life','ended my life',\n",
    "'want to end it','wanting to end it',\"wanna end it\",\"off myself\",\n",
    "    'going to end it','enough to end it all', 'ending it all',\n",
    "    \"dispose of my body\",\n",
    "    'going to die tonight', 'will die tonight', \n",
    "    'pull the trigger', 'pulling the trigger',\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "'''\n",
    "plan: 'suicidal plans', 'suicide plans', 'planning'\n",
    "\n",
    "attempts: 'i've tried multiple times'\n",
    "\n",
    "'''\n",
    "\n",
    "mental_health = [\n",
    "'in and out of the hospital','hospitalized',\n",
    "'psychiatr','mentally ill','mental problem','professional help',\"i'm not ok\",'diagnos','treatment','treated for','get help', 'getting help','therap','neuroti', 'stress','disorder',\n",
    "'mental health', 'craz','lunatic', 'insane','deranged','demented','unbalanced', 'unstable','suffer','pain',\n",
    "     'dysregulat', 'affective',   \n",
    "     'dull','bored',\n",
    "    'stress','overwhelm',\n",
    "    \"don't know what to do\",\"don know what to do\",\"dont know what to do\",\"idk what to do\", \"not knowing what to do\",\n",
    "'depress', 'dysthymia', 'MDD','sad','unhappy','exhaust', 'drain', 'ashamed', 'guilt','numb','useless','pathetic self','apath',\n",
    "'have no motivation','lack motivation',\"don't have motivation\",'keeps getting worse',\n",
    "\"i'm a failure\", \"fidgety\",\n",
    "'insomnia', \"can't sleep\", \"not sleeping\",\n",
    "    \"help\", \"need someone to talk\", 'talk to someone',\n",
    "    'angry',\n",
    "'narcissis','ego','selfish','self centered',\n",
    "'ptsd', 'trauma', 'flashback', 'trigger','nightmar','post traumatic stress disorder',\n",
    "'schizophren', 'hallucin', 'delus','hearing voic','heard voic','hear voic', 'paranoid','psycho','deliri',\n",
    " 'schizo',\n",
    "'anxiety','anxiou','antsy','anxieti', 'panic','obsess','ocd', 'worried', 'worry', 'nervous', 'phobi',\n",
    "'restless', 'irrita', 'annoy', 'impulsive', 'rambl',\n",
    "'autis', 'spectrum', \n",
    "'bipolar', 'manic', 'mania', 'lithium','episod',\n",
    "'restrict', 'purg', 'bing', 'calori', 'recover','vomit','fasted', 'fasting',\n",
    "'eating disorder', 'anorexi', 'bulimi',\n",
    "'adhd', 'adderal', 'vyvans',\n",
    "'medicat','lexapro','prozac','zoloft','valium','xanax',\n",
    "'cyclothymic',\n",
    " 'mutism',\n",
    " 'dysmorphic',\n",
    " 'hoard',\n",
    " 'trichotillo',\n",
    " 'dissociative',\n",
    " 'depersonaliz',\n",
    " 'ruminat',\n",
    " 'personality',\n",
    " 'antisocial',\n",
    " 'borderline',\n",
    " 'histrionic',\n",
    " 'avoidant',\n",
    " 'dependent',\n",
    " 'alogia',\n",
    " \n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def count_words(doc,tokens=[], normalize = True):\n",
    "    # Count number times a specific tokens (word, phrase) appears in the doc.\n",
    "    text = re.sub(\"[^\\w\\d'\\s]+\",'',doc.lower())    # remove punctuation except apostrophes because we need to search for things like \"don't want to live\"\n",
    "    counter = np.sum([text.count(token) for token in tokens])\n",
    "    if normalize:\n",
    "        \n",
    "        word_count = len(re.findall(r'\\w+', doc))\n",
    "        if word_count == 0:\n",
    "            assert counter == 0\n",
    "            return 0\n",
    "        normalized_counter = counter/word_count\n",
    "        return normalized_counter\n",
    "    else:\n",
    "        return counter\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_dir = './data/input/'\n",
    "docs = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_2021-07-20-23-12-27.csv', index_col = 0)['text'].values\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "lexicons = [financial_stress, domestic_stress_and_violence,loneliness_isolation, substance_use,suicidality_general,\n",
    "    suicidality_selfharm, suicidality_passive, suicidality_active,mental_health]\n",
    "\n",
    "lexicon_names = ['financial_stress',\n",
    " 'domestic_stress_and_violence',\n",
    " 'loneliness_isolation',\n",
    " 'substance_use',\n",
    " 'suicidality_general',\n",
    " 'suicidality_selfharm',\n",
    " 'suicidality_passive',\n",
    " 'suicidality_active',\n",
    " 'mental_health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame(np.hstack(lexicons)).value_counts()\n",
    "d[d>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "feature_vectors = []\n",
    "for i,lexicon in enumerate(lexicons):\n",
    "    print(i)\n",
    "    lexicon = [n.lower() for n in lexicon]\n",
    "    counters = [count_words(doc,tokens=lexicon, normalize=True) for doc in docs]\n",
    "    feature_vectors.append(counters)\n",
    "\n",
    "feature_vectors = pd.DataFrame(feature_vectors, index = lexicon_names).T\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors['text'] = docs\n",
    "pd.set_option('max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which ngrams were not captured at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexes = feature_vectors[feature_vectors.drop('text', axis=1).sum(axis=1)==0].index\n",
    "missed_sentences = feature_vectors.loc[indexes].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def count_vectorizer(docs, stop_words='english', ngram_range=(1,1), min_df = 1):\n",
    "\n",
    "\tvectorizer = CountVectorizer(\n",
    "\t    analyzer='word',\n",
    "\t    stop_words = stop_words, #None, #'english'\n",
    "\t    ngram_range=ngram_range,\n",
    "\t    min_df = min_df,\n",
    "\t)         #this is 1 per document, below, I compute total\n",
    "\tX = vectorizer.fit_transform(docs)\n",
    "\tngram_names = vectorizer.get_feature_names()\n",
    "\n",
    "\tngram_counts = pd.DataFrame(X.toarray(), columns=ngram_names)\n",
    "\tngram_counts_sum = pd.DataFrame(ngram_counts.sum()).sort_values(0)[::-1]\n",
    "\treturn X, ngram_counts, ngram_counts_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename_prefix = 'lonely2019'\n",
    "\n",
    "min_df = 2\n",
    "\n",
    "top_ngrams = []\n",
    "\n",
    "for ngram_range in range(2,8):\n",
    "\tif ngram_range  == 1:\n",
    "\t\t# top ones will just be stopwords\n",
    "\t\tstop_words = 'english'\n",
    "\telse:\n",
    "\t\tstop_words = None\n",
    "\n",
    "\tX, ngram_counts, ngram_counts_sum = count_vectorizer(missed_sentences, stop_words=stop_words, ngram_range=(ngram_range,ngram_range), min_df = min_df )\n",
    "\ttop_ngrams_i = ngram_counts_sum.reset_index()\n",
    "\ttop_ngrams_i.columns = [f'ngram_range_{ngram_range}', f'norm_freq_{ngram_range}']\n",
    "# \ttop_ngrams_i[f'norm_freq_{ngram_range}'] = top_ngrams_i[f'norm_freq_{ngram_range}']/top_ngrams_i[f'norm_freq_{ngram_range}'].sum()\n",
    "\ttop_ngrams_i = top_ngrams_i.reset_index(drop=True)\n",
    "\ttop_ngrams.append(top_ngrams_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ngrams_df = pd.concat(top_ngrams, ignore_index=False, axis=1)\n",
    "top_ngrams_df.to_csv(input_dir+'missed_ngrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after adding missing ngrams, concat new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))\n",
    "df = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(feature_vectors,on='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any duplicates?\n",
    "pd.DataFrame(df.text.value_counts()).iloc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = 'text',)\n",
    "print(df.shape)\n",
    "df.to_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[::1000, :].to_csv(input_dir+'screen_feature_extraction_random_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors.drop(['text'], axis=1).sum().sort_values()[::-1].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract features for second post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "subs = pd.read_csv(input_dir+'all_submissions_2021-07-20-23-11-56.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = subs.sort_values(['author', 'created_utc'])\n",
    "subs = subs[subs.subreddit == 'SuicideWatch']\n",
    "first_sw = []\n",
    "second_sw = []\n",
    "for i, author in enumerate(subs.author.unique()):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    subs_i = subs[subs.author ==author]\n",
    "    first_sw.append(\n",
    "        pd.DataFrame(subs_i.iloc[0,:]) #append first SW post\n",
    "    )\n",
    "    if subs_i.shape[0]>1:\n",
    "\n",
    "        second_sw.append(\n",
    "            pd.DataFrame(subs_i.iloc[1,:]) #append second SW post\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw_df = pd.concat(first_sw,axis=1).T\n",
    "second_sw_df = pd.concat(second_sw,axis=1).T\n",
    "second_sw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST PASSED: the original dataset contains all the first_sw_df posts: \n",
    "# original = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "# print(first_sw_df.shape, original.shape)\n",
    "# print(len(set(original.id.values)-set(first_sw_df.id.values)))\n",
    "# print(len(set(first_sw_df.id.values)-set(original.id.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sw_df = second_sw_df.reset_index(drop=True)\n",
    "text_all = []\n",
    "for title, body in zip(second_sw_df['title'].values, second_sw_df['selftext'].values):\n",
    "    title = title.strip()\n",
    "    if title.endswith('.') or title.endswith('?') or title.endswith('!'):\n",
    "        title += ' '\n",
    "    else:\n",
    "        title += '. '\n",
    "    text = title+str(body).strip()\n",
    "    text_all.append(text)\n",
    "\n",
    "text_all = pd.DataFrame(text_all)\n",
    "text_all\n",
    "        \n",
    "\n",
    "second_sw_df['text']  = text_all\n",
    "        \n",
    "second_sw_df[['text', 'title', 'selftext']]     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docs = second_sw_df.text.values\n",
    "\n",
    "\n",
    "feature_vectors = []\n",
    "for i,lexicon in enumerate(lexicons):\n",
    "    print(i)\n",
    "    lexicon = [n.lower() for n in lexicon]\n",
    "    counters = [count_words(doc,tokens=lexicon, normalize=True) for doc in docs]\n",
    "    feature_vectors.append(counters)\n",
    "\n",
    "feature_vectors = pd.DataFrame(feature_vectors, index = lexicon_names).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_vectors['text'] = docs\n",
    "pd.set_option('max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = second_sw_df.merge(feature_vectors,on='text')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(input_dir+'second_sw_submission_lexicons_2021-07-20-23-12-27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: maybe post to github\n",
    "\n",
    "\n",
    "\n",
    "# Reddit\n",
    "print('SW posts: ',df[df.subreddit =='SuicideWatch'].shape[0])\n",
    "print('Non SW posts: ',df[df.subreddit !='SuicideWatch'].shape[0])\n",
    "print('SW post rate: ',df[df.subreddit =='SuicideWatch'].shape[0]/df.shape[0])\n",
    "print('unique posters: ',len(df.author.unique()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.author.value_counts().describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset of first SW posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SITBI poster: person who posts SITBI-present\n",
    "commenter: someone who answers to a SITBI post\n",
    "\n",
    "1. Find first suicidal post and make sure first post is in 2018 if not remove.\n",
    "2. Then add a col \"relapse\" with 1 if they post SITB again and 2 if they do not. \n",
    "3. Discard prior posts: maybe keep them as covariates. Should we keep prior comments?\n",
    "4. Feature extraction: ngrams, features from comments, maybe features from profile, amount of commenters\n",
    "\n",
    "\n",
    "alternative covariates: instead being just the ngram, it could be a feature vector for each word. have they interacted before?\n",
    "simpler treatment: amount of responses\n",
    "alternative treatment: user, what type of user improves outcome\n",
    "alternative outcome: do they *not* post. This could be bad. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = pd.read_csv(input_dir+'all_submissions_2021-07-20-23-11-56.csv', index_col = 0)\n",
    "all_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs.created_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_subs.shape)\n",
    "all_subs = all_subs[~all_subs.created_utc.isnull()]\n",
    "print(all_subs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = all_subs[all_subs.created_utc.apply(pd.to_numeric, errors='coerce').notna()].dropna(subset=['created_utc'])\n",
    "all_subs.created_utc = all_subs.created_utc.astype(int)\n",
    "print(all_subs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [7,14,21,30,60,90,180,365]\n",
    "run = True\n",
    "re_save_df = True\n",
    "\n",
    "\n",
    "if run:\n",
    "    first_sw_post = []\n",
    "    posts_after_2018_dropped = []\n",
    "    no_sw_posts = 0\n",
    "    label = 'SuicideWatch' #maybe I want to add r/Depression or other mental health subreddits\n",
    "    total = all_subs.author.unique().shape[0]\n",
    "    print(total)\n",
    "\n",
    "    for i, author in enumerate(all_subs.author.unique()):\n",
    "        if i%50==0:\n",
    "            print(i)\n",
    "        subsequent_sw=0\n",
    "        # posts by author     \n",
    "        df_author = all_subs[all_subs.author==author]\n",
    "        df_author = df_author.sort_values('created_utc') #make sure its ordered by date (oldest first) because we want first suicidal post\n",
    "        suicide_posts = df_author[df_author['subreddit']==label]\n",
    "        if suicide_posts.shape[0]==0:\n",
    "            # no suicide_posts, I selected these authors from Reddit Mental Health Dataset SuicideWatch from 2018 and 2019, but their suicide posts were not found through the API?\n",
    "            no_sw_posts += 1\n",
    "            continue\n",
    "\n",
    "        # keep first post (they're sorted chronologically)    \n",
    "        suicide_posts_first = suicide_posts.drop_duplicates(subset = 'author',keep='first') \n",
    "        utc_first_sw_post = suicide_posts_first.created_utc.values[0]\n",
    "        id_first_sw_post = suicide_posts_first.id.values[0]\n",
    "\n",
    "        # TEST: PASSED\n",
    "        #double check if dates are in right order\n",
    "    #     assert df_author.sort_values('created_utc').equals(df_author) \n",
    "    #     print('\\n'+df_author.author.values[0])\n",
    "    #     print(date_first_sw_post.values[0])\n",
    "\n",
    "        #obtain all subsequent and prior posts #TODO: Double check this, because it says 4\n",
    "        df_author_subsequent = df_author[df_author.created_utc>utc_first_sw_post]\n",
    "        df_author_prior = df_author[df_author.created_utc<utc_first_sw_post]\n",
    "\n",
    "\n",
    "        if df_author.shape[0]>1:\n",
    "            # all subsequent posts\n",
    "            subreddits_counts = df_author_subsequent.subreddit.value_counts()\n",
    "            if label in subreddits_counts.index: #ToDo maybe include label or 'depression'\n",
    "                subsequent_sw = subreddits_counts[label]\n",
    "            else:\n",
    "                subsequent_sw = 0\n",
    "        else:\n",
    "            assert df_author.shape[0] == 1# there was only that first sw post, maybe some posts before, but none after.         \n",
    "            subsequent_sw = 0# did not post again after SW post\n",
    "\n",
    "\n",
    "        last_post_date = df_author.iloc[-1].created_utc\n",
    "        suicide_posts_first = suicide_posts_first.reset_index(drop=True)\n",
    "        suicide_posts_first.loc[0,'subsequent_sw']=subsequent_sw\n",
    "        suicide_posts_first.loc[0,'last_post_date']=last_post_date\n",
    "        suicide_posts_first.loc[0,'prior_posts']=df_author_prior.shape[0]\n",
    "        suicide_posts_first.loc[0,'subsequent_posts']=df_author_subsequent.shape[0]\n",
    "\n",
    "        for n in days:\n",
    "            cutoff_day_datetime = unixtimestamp_to_datetime(utc_first_sw_post)+timedelta(days=n)\n",
    "            cutoff_day_unix = int(datetime_to_unixtimestamp(cutoff_day_datetime))\n",
    "            df_author_subsequent_n = df_author[(df_author.created_utc>utc_first_sw_post)&(df_author.created_utc<=cutoff_day_unix)]\n",
    "            subreddits_counts_n = df_author_subsequent_n.subreddit.value_counts()\n",
    "            if label in subreddits_counts_n.index: #ToDo maybe include label or 'depression'\n",
    "                subsequent_sw_n = subreddits_counts_n[label]\n",
    "\n",
    "            else:\n",
    "                subsequent_sw_n= 0\n",
    "            suicide_posts_first.loc[0,f'subsequent_sw_{n}days']=subsequent_sw_n\n",
    "            suicide_posts_first.loc[0,f'subsequent_posts_{n}days']=df_author_subsequent_n.shape[0]\n",
    "\n",
    "        first_sw_post.append(suicide_posts_first)\n",
    "\n",
    "    first_sw_post = pd.concat(first_sw_post)\n",
    "    first_sw_post = first_sw_post.reset_index(drop=True)\n",
    "    first_sw_post['created_utc_readable'] = [unixtimestamp_to_readable(j) for j in first_sw_post.created_utc.values]\n",
    "\n",
    "    \n",
    "    if re_save_df:\n",
    "\n",
    "        timestamp ='{:%Y-%m-%d-%H-%M-%S}'.format(datetime.now())\n",
    "\n",
    "\n",
    "        first_sw_post.to_csv(input_dir+f'first_sw_submission_liwc2015_subsequent_posts_2021-07-20-23-12-27_{timestamp}.csv', )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resave all_subs without NaNs in created_utc. AND SEE WHAT OTHER NANS THERE OR PER COL\n",
    "all_subs.isna()2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resave all_subs without NaNs in created_utc. AND SEE WHAT OTHER NANS THERE OR PER COL\n",
    "all_subs.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_subs.to_csv(input_dir+'all_submissions_2021-07-20-23-11-56.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './data/input/'\n",
    "output_dir = './data/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "first_sw_post = pd.read_csv(input_dir+'first_sw_submission_liwc2015_subsequent_posts_lexicons_2021-07-20-23-12-27.csv', index_col = 0)\n",
    "comments = pd.read_csv(input_dir+'first_sw_comments_liwc2015_2021-07-20-18-52-35.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sw_posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('users without sw: ', no_sw_posts) #look into this, i selected them from Reddit Mental Health dataset\n",
    "print(f'Result: {first_sw_post.shape[0]} unique users and { len(first_sw_post.author.unique())} unique posts') \n",
    "# print('users with subsequent sw: ', first_sw_post.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_comments = pd.DataFrame(comments.author.value_counts().iloc[1:].describe()).T\n",
    "stats_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_stats = ['prior_posts','subsequent_posts','subsequent_sw', 'num_comments']\n",
    "stats = first_sw_post[columns_stats].describe().round(1).T\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_comments.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.loc[len(stats)] = stats_comments.values[0]\n",
    "stats = stats[['min', '25%', '50%', '75%', 'max']].astype(int)\n",
    "stats.index = ['Prior posts', 'All subsequent posts', 'Subsequent SW posts', 'Comments per post', 'Comments per commenter']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_counts = first_sw_post.prior_posts.value_counts().reset_index()\n",
    "sw_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    count = sw_counts[sw_counts['index']==i].prior_posts.values\n",
    "    print((count/df.shape[0]).round(3)*100, f'(N={count})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_count = first_sw_post.subsequent_posts.value_counts().reset_index()\n",
    "subsequent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4207/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [7,14,21,30,60,90,180,365]\n",
    "subsequent_df = []\n",
    "for n in days:\n",
    "    subsequent = pd.DataFrame(first_sw_post[f'subsequent_sw_{n}days'].value_counts())\n",
    "    subsequent_df.append(subsequent)\n",
    "\n",
    "subsequent_df = pd.concat(subsequent_df, axis=1)\n",
    "subsequent_df = subsequent_df.fillna(0)\n",
    "subsequent_df = subsequent_df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subsequent_df = subsequent_df.rename(columns = {'index':'posts'})\n",
    "subsequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_df[subsequent_df['posts']>7].sum()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subsequent_df[subsequent_df['posts']0].sum()[1:])\n",
    "(subsequent_df[subsequent_df['posts']!=0].sum()[1:]/df.shape[0]).round(3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subsequent_df[subsequent_df['posts']!=0].sum()[1:])\n",
    "(subsequent_df[subsequent_df['posts']!=0].sum()[1:]/df.shape[0]).round(3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=9\n",
    "rotation=90\n",
    "round_int = 3\n",
    "\n",
    "\n",
    "plt.figure(\n",
    "    figsize=(8,4), \n",
    "    dpi=300)\n",
    "\n",
    "# 1 or more\n",
    "x = days\n",
    "# y = subsequent_df[subsequent_df['posts']!=0].sum()[1:]\n",
    "y = (subsequent_df[subsequent_df['posts']!=0].sum()[1:]/df.shape[0]).round(round_int)\n",
    "\n",
    "plt.scatter(days,y)\n",
    "plt.plot(days,y, label = '1 or more')\n",
    "\n",
    "# 1 to 7\n",
    "for n in range(1,7):\n",
    "#     y = subsequent_df[subsequent_df['posts']==n].values[0][1:]\n",
    "    y = (subsequent_df[subsequent_df['posts']==n].values[0][1:]/df.shape[0]).round(round_int)\n",
    "    \n",
    "    plt.scatter(days,y)\n",
    "    plt.plot(days,y, label = n)\n",
    "    \n",
    "# y = subsequent_df[subsequent_df['posts']>=7].sum()[1:]\n",
    "# plt.scatter(days,y)\n",
    "# plt.plot(days,y, label = '7 or more')\n",
    "\n",
    "\n",
    "\n",
    "l = plt.legend(bbox_to_anchor=(1.05, 1), title = 'Subsequent\\nr/SuicideWatch\\nposts', framealpha=1)\n",
    "plt.setp(l.get_title(), multialignment='center')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Proportion of users')\n",
    "plt.xticks(ticks=days, labels=days, rotation=rotation, size=size)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir+'figures/subsequent_sw_posts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(\n",
    "#     figsize=(3,2), \n",
    "    dpi=300)\n",
    "\n",
    "for n in [0]:\n",
    "    y = subsequent_df[subsequent_df['posts']==n].values[0][1:]\n",
    "    plt.scatter(days,y)\n",
    "    plt.plot(days,y, label = n)\n",
    "\n",
    "plt.legend(\n",
    "#     bbox_to_anchor=(1.05, 1), \n",
    "    title = 'Subsequent\\nr/SuicideWatch\\nposts')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Users')\n",
    "plt.xticks(ticks=days, labels=days, rotation=rotation, size=size)\n",
    "plt.ylim(0,21100)\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_posts = first_sw_post.prior_posts\n",
    "prior_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = pd.DataFrame(first_sw_post.prior_posts.value_counts())\n",
    "\n",
    "prior = prior.sort_index().reset_index()\n",
    "prior.columns = ['Prior posts', 'Users']\n",
    "prior = prior.astype(int)\n",
    "prior['Users %'] = (prior['Users']/prior['Users'].sum()).round(3)\n",
    "\n",
    "prior.iloc[:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_posts.describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(prior_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(prior_posts, bins = int(np.max(prior_posts)))\n",
    "plt.xlim(-1,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(prior_posts.describe().astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent = pd.DataFrame(first_sw_post.subsequent_posts.value_counts())\n",
    "subsequent_posts = pd.DataFrame(first_sw_post.subsequent_posts)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.histplot(subsequent_posts, bins = int(np.max(subsequent_posts))+1, legend=False)\n",
    "\n",
    "plt.xlim(-1,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
